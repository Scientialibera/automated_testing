{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Classes')\n",
    "from AzureSearch import Config, AzureSearch\n",
    "import pandas as pd\n",
    "import openai\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.search.documents.indexes.models import (\n",
    "    VectorSearch,\n",
    "    HnswVectorSearchAlgorithmConfiguration\n",
    ")\n",
    "\n",
    "config = Config(path=\"../Classes/environment.env\")\n",
    "cred = DefaultAzureCredential()\n",
    "token = cred.get_token(\"https://cognitiveservices.azure.com/.default\")\n",
    "\n",
    "openai.api_key = token.token\n",
    "openai.api_version = config.openai_api_version\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_base = config.openai_api_base\n",
    "\n",
    "vector_search = VectorSearch(\n",
    "    algorithm_configurations=[\n",
    "        HnswVectorSearchAlgorithmConfiguration(\n",
    "            name=config.vector_config_name,\n",
    "            kind=\"hnsw\",\n",
    "            parameters={\n",
    "                \"m\": 4,\n",
    "                \"efConstruction\": 400,\n",
    "                \"efSearch\": 500,\n",
    "                \"metric\": \"cosine\"\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "def generate_grounded_answer(query, content, system, reminder, config=config):\n",
    "    # Format the user message using the given query and content\n",
    "    user_message = f\"Source:\\n{content}\\n Answer the User question: {query}\"\n",
    "\n",
    "    # Construct the message to be sent to the model\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"\"\"{system}\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"{user_message}\\n{reminder}\"\"\"}\n",
    "    ]\n",
    "\n",
    "    # Request a response from the model\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=config.gpt,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    \n",
    "    # Extract and return the model's reply\n",
    "    bot_reply = response['choices'][0]['message']['content']\n",
    "    return bot_reply\n",
    "\n",
    "def concatenate_sources(results):\n",
    "    # Sort the array by '@search.score' in ascending order\n",
    "    sorted_results = sorted(results, key=lambda x: x['@search.score'])\n",
    "\n",
    "    # Initialize the 'sources' string\n",
    "    sources = \"\"\n",
    "\n",
    "    # Loop through the sorted results and format the string\n",
    "    for item in sorted_results:\n",
    "        title = item.get('title', 'No Title')\n",
    "        citation_id = item.get('id', 'No ID')\n",
    "        content = item.get('content', 'No Content')\n",
    "        sources += f\"{title}\\nCitation: {citation_id}\\n{content}\\n\\n\"\n",
    "\n",
    "    return sources\n",
    "\n",
    "def process_queries_and_generate_responses(queries_titles, index_name, k, prompts, config=config):\n",
    "    azure_search = AzureSearch(config, index_name)\n",
    "    responses = []  # This list will store the final responses for each query and title\n",
    "    queries_titles_df = pd.read_csv(queries_titles, encoding='utf-8')\n",
    "    # Iterate over each row in the queries and titles DataFrame\n",
    "    for _, row in queries_titles_df.iterrows():\n",
    "        query = row['query']\n",
    "        title = row['title']\n",
    "        filter = f\"title eq '{title}'\"\n",
    "        # Use the mock Azure search function to get the top result for the query and title\n",
    "        search_results = azure_search.vector_search(query, filter, k, \"id,title,content\")\n",
    "        # If there are results, get the content of the top result\n",
    "        content = [i for i in search_results]\n",
    "        \n",
    "        # For each prompt in the prompts DataFrame, generate a response using the mock OpenAI function\n",
    "        prompt_responses = {}\n",
    "        prompts_df = pd.read_csv(prompts, encoding='utf-8')\n",
    "        for index, prompt_row in prompts_df.iterrows():\n",
    "            prompt = prompt_row['prompt']\n",
    "            reminder = prompt_row['reminder']\n",
    "            ai_response = generate_grounded_answer(query, content, prompt, reminder, config)\n",
    "            # Assuming you want the row num to start from 1\n",
    "            column_name = f'prompt_{index + 1}'  # Adding 1 if the DataFrame index starts at 0\n",
    "            prompt_responses[column_name] = ai_response\n",
    "\n",
    "        # Compile the query, title, and all prompt responses into a single record\n",
    "        record = {\n",
    "            'query': query,\n",
    "            'title': title,\n",
    "            **prompt_responses  # Add the prompt responses with dynamic keys\n",
    "        }\n",
    "        responses.append(record)\n",
    "\n",
    "    # Create a DataFrame from the responses\n",
    "    final_df = pd.DataFrame(responses)\n",
    "\n",
    "    return final_df\n",
    "\n",
    "process_queries_and_generate_responses('./test_csv.txt', config.index_name, config.top_k, './test_csv_prompt.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
